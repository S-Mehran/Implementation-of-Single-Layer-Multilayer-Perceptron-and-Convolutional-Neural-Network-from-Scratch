{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34877,"sourceType":"datasetVersion","datasetId":27352}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:25:46.270571Z","iopub.execute_input":"2025-03-05T18:25:46.270989Z","iopub.status.idle":"2025-03-05T18:25:47.620810Z","shell.execute_reply.started":"2025-03-05T18:25:46.270948Z","shell.execute_reply":"2025-03-05T18:25:47.619486Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mnist-in-csv/mnist_test.csv\n/kaggle/input/mnist-in-csv/mnist_train.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\nimport random\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport time\nimport os\nfrom scipy.signal import correlate2d\nfrom skimage.measure import block_reduce\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:27:09.157323Z","iopub.execute_input":"2025-03-05T18:27:09.157689Z","iopub.status.idle":"2025-03-05T18:27:09.372053Z","shell.execute_reply.started":"2025-03-05T18:27:09.157660Z","shell.execute_reply":"2025-03-05T18:27:09.370858Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\nclass CNN:\n    def __init__(self, input_size=(28, 28), num_filters=8, filter_size=3, pool_size=2, num_classes=10, learning_rate=0.01):\n        self.input_size = input_size\n        self.num_filters = num_filters\n        self.filter_size = filter_size\n        self.pool_size = pool_size\n        self.num_classes = num_classes\n        self.lr = learning_rate\n\n        # Initialize filters and weights\n        self.filters = np.random.randn(num_filters, filter_size, filter_size) * 0.1\n        self.fc_weights = np.random.randn(num_filters * ((input_size[0] - filter_size + 1) // pool_size) * \n                                          ((input_size[1] - filter_size + 1) // pool_size), num_classes) * 0.1\n        self.fc_bias = np.zeros((1, num_classes))\n\n    def convolve2D(self, image, kernel):\n        return correlate2d(image, kernel, mode='valid')\n        # kernel_size = kernel.shape[0]\n        # output_size = (image.shape[0] - kernel_size + 1, image.shape[1] - kernel_size + 1)\n        # output = np.zeros(output_size)\n\n        # for i in range(output_size[0]):\n        #     for j in range(output_size[1]):\n        #         region = image[i:i + kernel_size, j:j + kernel_size]\n        #         output[i, j] = np.sum(region * kernel)\n\n        # return output\n\n    def max_pooling(self, feature_map, size):\n        h, w = feature_map.shape\n        output_size = (h // size, w // size)\n        output = np.zeros(output_size)\n        self.pool_cache = {}\n\n        for i in range(output_size[0]):\n            for j in range(output_size[1]):\n                region = feature_map[i * size:(i + 1) * size, j * size:(j + 1) * size]\n                max_val = np.max(region)\n                max_pos = np.unravel_index(np.argmax(region), region.shape)\n                output[i, j] = max_val\n                self.pool_cache[(i, j)] = (i * size + max_pos[0], j * size + max_pos[1])  # Store relative positions\n\n        return output\n\n    def sigmoid(self, x):\n        x = np.clip(x, -500, 500)  # Prevent overflow\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)\n\n    def softmax(self, x):\n        exps = np.exp(x - np.max(x))\n        return exps / np.sum(exps, axis=1, keepdims=True)\n\n    def cross_entropy_loss(self, probs, label):\n        probs = np.clip(probs, 1e-10, 1)  # Prevent log(0)\n        return -np.log(probs[0, label])  # Corrected label indexing\n\n    def forward(self, image):\n        self.input_image = image\n        self.feature_maps = np.array([self.convolve2D(image, kernel) for kernel in self.filters])\n        self.sigmoid_maps = np.array([self.sigmoid(x) for x in self.feature_maps])\n        self.pooled_maps = np.array([self.max_pooling(fm, self.pool_size) for fm in self.sigmoid_maps])\n\n        self.flattened = self.pooled_maps.flatten().reshape(1, -1)\n        self.scores = np.dot(self.flattened, self.fc_weights) + self.fc_bias\n        self.probs = self.softmax(self.scores)\n\n        return self.probs\n\n    def backward(self, label):\n        d_scores = self.probs.copy()  # Don't modify self.probs\n        d_scores[0, label] -= 1  # Compute gradient of loss w.r.t. scores\n\n        d_fc_weights = np.dot(self.flattened.T, d_scores)\n        d_fc_bias = np.sum(d_scores, axis=0, keepdims=True)\n        d_flattened = np.dot(d_scores, self.fc_weights.T)\n\n        d_pooled = d_flattened.reshape(self.pooled_maps.shape)\n\n        # Backprop through pooling\n        d_sigmoid_maps = np.zeros_like(self.sigmoid_maps)\n        for i in range(self.num_filters):\n            for (py, px), (y, x) in self.pool_cache.items():\n                d_sigmoid_maps[i, y, x] = d_pooled[i, py, px]  # Use stored pooling indices\n\n        # Backprop through sigmoid\n        d_feature_maps = d_sigmoid_maps * self.sigmoid_derivative(self.sigmoid_maps)\n\n        # Backprop through convolution\n        d_filters = np.zeros_like(self.filters)\n        for i in range(self.num_filters):\n            for y in range(self.filter_size):\n                for x in range(self.filter_size):\n                    region = self.input_image[y:y + d_feature_maps.shape[1], x:x + d_feature_maps.shape[2]]\n                    d_filters[i, y, x] = np.sum(region * d_feature_maps[i])\n\n        # Gradient clipping to prevent instability\n        d_fc_weights = np.clip(d_fc_weights, -1, 1)\n        d_fc_bias = np.clip(d_fc_bias, -1, 1)\n        d_filters = np.clip(d_filters, -1, 1)\n\n        # Update weights\n        self.fc_weights -= self.lr * d_fc_weights\n        self.fc_bias -= self.lr * d_fc_bias\n        self.filters -= self.lr * d_filters\n\n\n    def predict(self, image):\n        \"\"\" Returns predicted class label \"\"\"\n        output = self.forward(image)\n        return np.argmax(output)\n\n    def train(self, dataset, labels, epochs=10, batch_size=32):\n        num_samples = dataset.shape[0]\n        history = {'loss': [], 'f1_score': [], 'training_time': []}\n        \n        for epoch in range(epochs):\n            start_time = time.time()\n            loss = 0\n            num_batches = num_samples//batch_size\n            all_preds = []\n            all_labels = []\n            \n            with tqdm(total=num_batches, desc=f\"Epoch {epoch + 1}/{epochs}\") as pbar:\n                for i in range(0, num_samples, batch_size):\n                    X_batch = dataset[i:i+batch_size]\n                    y_batch = labels[i:i+batch_size]\n                    batch_loss = 0\n            \n                    for image, label in zip(X_batch, y_batch):\n                        #image, label = dataset[i], labels[i]\n                        output = self.forward(image)\n                        batch_loss += self.cross_entropy_loss(output, label)\n                        self.backward(label)\n\n                        pred_label = np.argmax(output)\n                        all_preds.append(pred_label)\n                        all_labels.append(label)\n                    loss+= batch_loss/len(X_batch)\n                    pbar.update(1)\n\n            avg_loss = loss / len(dataset)\n            f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n            epoch_time = time.time() - start_time\n\n            history['loss'].append(avg_loss)\n            history['f1_score'].append(f1)\n            history['training_time'].append(epoch_time)\n\n            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n        return history\n\n\n    def evaluate(self, dataset, labels):\n        preds = np.array([self.predict(image) for image in dataset])\n        labels = np.array(labels)\n        \n        accuracy = np.mean(preds == labels)\n        f1 = f1_score(labels, preds, average=\"weighted\")\n        conf_matrix = confusion_matrix(labels, preds)\n\n        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n        print(f\"F1 Score: {f1:.4f}\")\n        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n\n        return {\"accuracy\": accuracy, \"f1_score\": f1, \"confusion_matrix\": conf_matrix}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:33:14.782262Z","iopub.execute_input":"2025-03-05T19:33:14.782712Z","iopub.status.idle":"2025-03-05T19:33:14.811220Z","shell.execute_reply.started":"2025-03-05T19:33:14.782670Z","shell.execute_reply":"2025-03-05T19:33:14.809807Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Load MNIST dataset (CSV format)\ntrain_data = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\ntest_data = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n\n\n# Split features and labels\nX_train = train_data.iloc[0:10000, 1:].values  # Pixels\ny_train = train_data.iloc[0:10000, 0].values   # Labels\n\nX_test = test_data.iloc[0:10000, 1:].values\ny_test = test_data.iloc[0:10000, 0].values\n\n# Normalize pixel values (0 to 1)\nX_train = X_train / 255.0\nX_test = X_test / 255.0\n\n# Reshape to (28, 28) for CNN input\nX_train = X_train.reshape(-1, 28, 28)\nX_test = X_test.reshape(-1, 28, 28)\n\n# Define number of classes\nnum_classes = 10\n\n# One-hot encode labels\ndef one_hot_encode(y, num_classes):\n    encoded = np.zeros((len(y), num_classes))\n    encoded[np.arange(len(y)), y] = 1\n    return encoded\n\ny_train_one_hot = one_hot_encode(y_train, num_classes)\ny_test_one_hot = one_hot_encode(y_test, num_classes)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:57:43.381676Z","iopub.execute_input":"2025-03-05T18:57:43.382087Z","iopub.status.idle":"2025-03-05T18:57:47.984091Z","shell.execute_reply.started":"2025-03-05T18:57:43.382052Z","shell.execute_reply":"2025-03-05T18:57:47.982707Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"cnn = CNN(input_size=(28, 28), num_filters=8, filter_size=3, pool_size=2, num_classes=10, learning_rate=0.01)\n\n# Train CNN on MNIST\nhistory = cnn.train(X_train, y_train, epochs=10)  # Using only 5000 samples for faster training\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:37:11.629117Z","iopub.execute_input":"2025-03-05T19:37:11.629506Z","iopub.status.idle":"2025-03-05T20:08:13.131493Z","shell.execute_reply.started":"2025-03-05T19:37:11.629475Z","shell.execute_reply":"2025-03-05T20:08:13.130352Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 313it [03:04,  1.69it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.0282\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 313it [03:06,  1.68it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.0143\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 313it [03:05,  1.69it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 0.0132\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 313it [03:06,  1.68it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 0.0126\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 313it [03:05,  1.69it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 0.0121\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 313it [03:06,  1.68it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 0.0118\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 313it [03:06,  1.68it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 0.0114\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 313it [03:07,  1.67it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 0.0110\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 313it [03:07,  1.67it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 0.0106\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 313it [03:06,  1.68it/s]                         ","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 0.0101\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\nperformance = cnn.evaluate(X_test, y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T20:08:13.132708Z","iopub.execute_input":"2025-03-05T20:08:13.133096Z","iopub.status.idle":"2025-03-05T20:11:02.823534Z","shell.execute_reply.started":"2025-03-05T20:08:13.133066Z","shell.execute_reply":"2025-03-05T20:11:02.822460Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 90.52%\nF1 Score: 0.9056\nConfusion Matrix:\n[[ 955    0   10    1    0    5    3    1    5    0]\n [   0 1098   15    3    0    1    2    0   16    0]\n [   4    5  975    5    6    1    3    7   22    4]\n [   3    0   60  898    2   14    0    6   21    6]\n [   2    7   18    1  901    1    9    0   16   27]\n [  11    6   17   45    4  758    5    3   39    4]\n [  11    3   52    1   33   17  834    1    6    0]\n [   1   20   64   11   12    2    0  888    6   24]\n [   7    8   26   18   10   16    1    2  878    8]\n [   9   11    7   14   61    6    0   17   17  867]]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(\"History:\\n\", history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T20:20:34.777592Z","iopub.execute_input":"2025-03-05T20:20:34.777959Z","iopub.status.idle":"2025-03-05T20:20:34.783855Z","shell.execute_reply.started":"2025-03-05T20:20:34.777928Z","shell.execute_reply":"2025-03-05T20:20:34.782658Z"}},"outputs":[{"name":"stdout","text":"History:\n {'loss': [0.028240715566814886, 0.014333379747239642, 0.013206571228700928, 0.012591525695632561, 0.012141445638313389, 0.011757770604437778, 0.011388234525467377, 0.010994051605974588, 0.010550400096438888, 0.010050936174886463], 'f1_score': [0.713698450703459, 0.8599361914492653, 0.8748145480959454, 0.8810671610034746, 0.8863219831635631, 0.8898598391662282, 0.8945812403945363, 0.8986032019612461, 0.9019000755081572, 0.9061251273591663], 'training_time': [184.7057752609253, 186.10485792160034, 185.55939626693726, 186.48927998542786, 185.32050371170044, 186.5687997341156, 186.1742067337036, 187.11037826538086, 187.37517499923706, 186.08647418022156]}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"total = 0\n\nfor hist in history['training_time']:\n    total+= hist\n\nprint(total)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T20:24:28.949862Z","iopub.execute_input":"2025-03-05T20:24:28.950250Z","iopub.status.idle":"2025-03-05T20:24:28.956075Z","shell.execute_reply.started":"2025-03-05T20:24:28.950210Z","shell.execute_reply":"2025-03-05T20:24:28.954751Z"}},"outputs":[{"name":"stdout","text":"1861.4948470592499\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}