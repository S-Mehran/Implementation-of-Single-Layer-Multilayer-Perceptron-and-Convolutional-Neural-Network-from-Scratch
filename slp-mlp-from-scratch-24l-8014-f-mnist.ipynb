{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9243,"sourceType":"datasetVersion","datasetId":2243}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:54:29.151140Z","iopub.execute_input":"2025-03-05T17:54:29.151419Z","iopub.status.idle":"2025-03-05T17:54:29.624441Z","shell.execute_reply.started":"2025-03-05T17:54:29.151394Z","shell.execute_reply":"2025-03-05T17:54:29.623382Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fashionmnist/t10k-labels-idx1-ubyte\n/kaggle/input/fashionmnist/t10k-images-idx3-ubyte\n/kaggle/input/fashionmnist/fashion-mnist_test.csv\n/kaggle/input/fashionmnist/fashion-mnist_train.csv\n/kaggle/input/fashionmnist/train-labels-idx1-ubyte\n/kaggle/input/fashionmnist/train-images-idx3-ubyte\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport random\nimport time\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:09:29.980754Z","iopub.execute_input":"2025-03-05T18:09:29.981264Z","iopub.status.idle":"2025-03-05T18:09:30.722137Z","shell.execute_reply.started":"2025-03-05T18:09:29.981225Z","shell.execute_reply":"2025-03-05T18:09:30.721181Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class SingleLayerNN:\n    def __init__(self, input_size, output_size, learning_rate=0.01):\n        self.weights = np.random.randn(input_size, output_size) * 0.01  # Small random values\n        self.bias = np.zeros((1, output_size))\n        self.learning_rate = learning_rate\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)  # Derivative of sigmoid\n\n    def forward(self, X):\n        self.input = X\n        self.z = np.dot(X, self.weights) + self.bias  # Linear transformation\n        self.output = self.sigmoid(self.z)  # Apply activation\n        return self.output\n\n    def backward(self, y_true):\n        error = self.output - y_true  # Error in prediction\n        d_output = error * self.sigmoid_derivative(self.output)  # Delta (Gradient)\n        \n        d_weights = np.dot(self.input.T, d_output)  # Weight gradient\n        d_bias = np.sum(d_output, axis=0, keepdims=True)  # Bias gradient\n\n        # Update weights and bias using gradient descent\n        self.weights -= self.learning_rate * d_weights\n        self.bias -= self.learning_rate * d_bias\n\n        loss = np.mean(error**2)  # Mean Squared Error\n        return loss\n\n    def train(self, X, y, epochs=100, X_test=None, y_test=None):\n        start_time = time.time()\n        \n        for epoch in range(epochs):\n            output = self.forward(X)\n            loss = self.backward(y)\n            \n            if epoch % 10 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n        \n        end_time = time.time()\n        print(f\"Total Training Time: {end_time - start_time:.2f} seconds\")\n        \n        if X_test is not None and y_test is not None:\n            self.evaluate(X_test, y_test)\n\n    def predict(self, X):\n        return (self.forward(X) > 0.5).astype(int)  # Convert probabilities to class labels\n    \n    def evaluate(self, X_test, y_test):\n        y_pred = self.predict(X_test)\n        \n        \n        accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n        f1 = f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='macro')\n        conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n\n        print(f\"Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}\")\n        print(\"Confusion Matrix:\\n\", conf_matrix)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:17:39.235628Z","iopub.execute_input":"2025-03-05T18:17:39.236101Z","iopub.status.idle":"2025-03-05T18:17:39.247649Z","shell.execute_reply.started":"2025-03-05T18:17:39.236065Z","shell.execute_reply":"2025-03-05T18:17:39.246464Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/fashionmnist/fashion-mnist_train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/fashionmnist/fashion-mnist_test.csv\")\n\n# **2. Preprocess Data**\nX_train = train_data.iloc[:, 1:].values / 255.0  \ny_train = train_data.iloc[:, 0].values.reshape(-1, 1)  \n\nX_test = test_data.iloc[:, 1:].values / 255.0  \ny_test = test_data.iloc[:, 0].values.reshape(-1, 1)  \n\n# **3. One-Hot Encoding for Labels**\nencoder = OneHotEncoder(sparse=False)\ny_train_onehot = encoder.fit_transform(y_train)\ny_test_onehot = encoder.transform(y_test)\n\n# **4. Initialize Model**\ninput_size = X_train.shape[1]  \noutput_size = y_train_onehot.shape[1]  \nmodel = SingleLayerNN(input_size, output_size, learning_rate=0.1)\n\n# **5. Train Model**\nmodel.train(X_train, y_train_onehot, epochs=100, X_test=X_test, y_test=y_test_onehot)\n\n# **6. Evaluate Model**\n# y_pred = model.predict(X_test)\n# y_pred = np.argmax(model.forward(X_test), axis=1)  # Convert one-hot predictions to class labels\n# y_test = y_test.ravel()  # Ensure y_test is a 1D array\n\n# accuracy = accuracy_score(y_test, y_pred)\n# f1 = f1_score(y_test, y_pred, average='macro')\n# #conf_matrix = confusion_matrix(y_test, y_pred)\n\n# print(f\"Accuracy: {accuracy:.4f}\")\n# print(f\"F1 Score: {f1:.4f}\")\n#print(\"Confusion Matrix:\")\n#print(conf_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:17:43.413688Z","iopub.execute_input":"2025-03-05T18:17:43.414071Z","iopub.status.idle":"2025-03-05T18:18:05.920062Z","shell.execute_reply.started":"2025-03-05T18:17:43.414040Z","shell.execute_reply":"2025-03-05T18:18:05.919013Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Loss: 0.2479\nEpoch 10, Loss: 0.1000\nEpoch 20, Loss: 0.1000\nEpoch 30, Loss: 0.1000\nEpoch 40, Loss: 0.1000\nEpoch 50, Loss: 0.1000\nEpoch 60, Loss: 0.1000\nEpoch 70, Loss: 0.1000\nEpoch 80, Loss: 0.1000\nEpoch 90, Loss: 0.1000\nTotal Training Time: 18.31 seconds\nAccuracy: 0.1000, F1-score: 0.0182\nConfusion Matrix:\n [[1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class Multilayer_Perceptron(object):\n    def __init__(self, sizes):\n        self.layers = len(sizes)\n        self.sizes = sizes\n        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n        self.weights = [np.random.randn(y,x) for x, y in zip(sizes[:-1], sizes[1:])]\n\n\n    def feedforward(self, a):\n        for b, w in zip(self.biases, self.weights):\n            a = self.sigmoid(np.dot(w,a)+b)\n        return a\n\n    def backpropagation(self, x, y):\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        activation = x\n        activation_list = [x]\n        z_list = []\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation)+b\n            z_list.append(z)\n            activation = self.sigmoid(z)\n            activation_list.append(activation)\n\n        delta = self.cost_derivative(activation_list[-1], y) * self.sigmoid_prime(z_list[-1]) \n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activation_list[-2].transpose())\n\n        for layer in range(2, self.layers):\n            z = z_list[-layer]\n            sp = self.sigmoid_prime(z)\n            delta = np.dot(self.weights[-layer+1].transpose(), delta)*sp\n            nabla_b[-layer] = delta\n            nabla_w[-layer] = np.dot(delta, activation_list[-layer-1].transpose())\n\n        return (nabla_b, nabla_w)\n\n\n    def update_mini_batch(self, mini_batch, eta):\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w = self.backpropagation(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n\n        self.weights = [w-(eta/len(mini_batch))*nw for w,nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n\n\n    def Stochastic_Gradient_Descent(self, epochs, training_data, mini_batch_size, eta, test_data=None):\n        n= len(training_data)\n        \n        if test_data:\n            test_n = len(test_data)\n            #n = len(training_data)\n        start_time = time.time()\n        for j in range(epochs):\n            random.shuffle(training_data)\n            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(mini_batch, eta)\n\n            if test_data:\n                #print('here')\n                #print(\"Epoch{0}: {1}/{2}\".format(j, self.evaluate(test_data), test_n))\n                accuracy, f1, conf_matrix = self.evaluate(test_data)\n                print(f\"Epoch {j}: Accuracy = {accuracy:.4f}, F1-score = {f1:.4f}\")\n                print(\"Confusion Matrix:\\n\", conf_matrix)\n       \n            else:\n                #print('code is here')\n                print(\"Epoch{0} complete\".format(j))\n        end_time = time.time()\n        total_time = end_time-start_time\n        print(f\"Total Training Time: {end_time - start_time:.2f} seconds\")\n\n    def cost_derivative(self, output_activations, y):\n        return (output_activations-y)\n    \n    def sigmoid(self, z):\n        return 1/(1+np.exp(-z))\n\n    def sigmoid_prime(self, z):\n        result = self.sigmoid(z)*(1-self.sigmoid(z))\n        return result\n\n\n    def evaluate(self, test_data):\n        y_pred = []\n        y_true = []\n        for x, y in test_data:\n            predicted = np.argmax(self.feedforward(x))\n            actual = np.argmax(y)\n            y_pred.append(predicted)\n            y_true.append(actual)\n        #test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n        #return sum(int(x == y) for (x, y) in test_results)\n        accuracy = accuracy_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred, average=\"macro\")\n        conf_matrix = confusion_matrix(y_true, y_pred)\n\n        return accuracy, f1, conf_matrix\n\n\n        \n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:19:16.338906Z","iopub.execute_input":"2025-03-05T18:19:16.339307Z","iopub.status.idle":"2025-03-05T18:19:16.356383Z","shell.execute_reply.started":"2025-03-05T18:19:16.339276Z","shell.execute_reply":"2025-03-05T18:19:16.355143Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"training_data = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\ntest_data = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')\ny_train = training_data['label']\nX_train = training_data.drop(['label'], axis=1)\ny_test = test_data['label']\nX_test = test_data.drop(['label'], axis=1)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:19:25.892499Z","iopub.execute_input":"2025-03-05T18:19:25.892881Z","iopub.status.idle":"2025-03-05T18:19:29.992021Z","shell.execute_reply.started":"2025-03-05T18:19:25.892835Z","shell.execute_reply":"2025-03-05T18:19:29.990789Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X_train = X_train.values/255.0\nX_test = X_test.values/255.0\n\n\n# Convert labels to one-hot encoding\ndef one_hot_encode(y, num_classes=10):\n    one_hot = np.zeros((len(y), num_classes))\n    one_hot[np.arange(len(y)), y] = 1\n    return one_hot\n\ny_train_oh = one_hot_encode(y_train)\ny_test_oh = one_hot_encode(y_test)\n\n# Reshape inputs to column vectors\nX_train = [x.reshape(-1, 1) for x in X_train]\ny_train_oh = [y.reshape(-1, 1) for y in y_train_oh]\nX_test = [x.reshape(-1, 1) for x in X_test]\ny_test_oh = [y.reshape(-1, 1) for y in y_test_oh]\n\n# Combine into tuples\ntraining_data = list(zip(X_train, y_train_oh))\ntest_data = list(zip(X_test, y_test_oh))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:20:05.799981Z","iopub.execute_input":"2025-03-05T18:20:05.800336Z","iopub.status.idle":"2025-03-05T18:20:06.115906Z","shell.execute_reply.started":"2025-03-05T18:20:05.800309Z","shell.execute_reply":"2025-03-05T18:20:06.115026Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Define model architecture: [784 input nodes, 128 hidden nodes, 10 output nodes]\nmlp = Multilayer_Perceptron([784, 16, 16, 10])\n\n# Train the model\nmlp.Stochastic_Gradient_Descent(\n    epochs=10, \n    training_data=training_data, \n    mini_batch_size=32, \n    eta=0.1, \n    test_data=test_data\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:20:11.228533Z","iopub.execute_input":"2025-03-05T18:20:11.228895Z","iopub.status.idle":"2025-03-05T18:21:38.511667Z","shell.execute_reply.started":"2025-03-05T18:20:11.228841Z","shell.execute_reply":"2025-03-05T18:21:38.510744Z"}},"outputs":[{"name":"stdout","text":"Epoch 0: Accuracy = 0.3332, F1-score = 0.2591\nConfusion Matrix:\n [[ 21 203 255 186 214   0  78   7  35   1]\n [  9 859  31  38  24   0  27   6   6   0]\n [  0  81 138  69 597   0  89  10  13   3]\n [ 21 282  82 369 135   0  62   3  46   0]\n [  3  53 105 137 557   0  92  28  25   0]\n [ 17 128  24  51  32   0   7 577  87  77]\n [ 10 165 177 100 387   0 127  11  22   1]\n [  0  19   0   2   7   0   0 949  11  12]\n [  6 148  27  37 186   0  18 304 268   6]\n [  1  79  31  18 141   0  18 619  49  44]]\nEpoch 1: Accuracy = 0.4888, F1-score = 0.4385\nConfusion Matrix:\n [[609  66  49 119  87   0  39   1  28   2]\n [ 58 884  10  26   5   0  14   1   2   0]\n [ 48  26 192  38 599   0  79   2  10   6]\n [234 165  12 468  67   0  16   0  38   0]\n [131  25  81  88 589   0  50   2  29   5]\n [ 31  63  25  12  22   0   4 449  78 316]\n [254  58 147  64 354   0  86   1  33   3]\n [  0   4   0   0   3   0   0 837   3 153]\n [ 17  29  36  18 120   0  21  85 449 225]\n [  1   9  41   2  24   0   2 106  41 774]]\nEpoch 2: Accuracy = 0.5552, F1-score = 0.5034\nConfusion Matrix:\n [[709  29  48 104  58   0  23   3  23   3]\n [ 40 896  12  37   5   0   7   1   2   0]\n [ 30   6 328  27 550   0  35   2  20   2]\n [176  79  26 621  55   0  10   0  33   0]\n [ 65  20 146  83 626   0  26   0  33   1]\n [ 25  25  40   2  15   0   9 431 127 326]\n [234  25 209  82 353   0  58   0  36   3]\n [  0   2   0   0   1   0   0 839  16 142]\n [  5   6  70  13  80   0   7  33 626 160]\n [  1   7  30   0   6   0   1  73  33 849]]\nEpoch 3: Accuracy = 0.5897, F1-score = 0.5436\nConfusion Matrix:\n [[747  20  53  89  22   0  39   3  26   1]\n [ 28 900  15  36   4   0  15   1   1   0]\n [ 22   3 553  13 321   0  68   1  15   4]\n [123  47  65 689  12   0  35   0  29   0]\n [ 23  16 384  74 426   0  54   0  23   0]\n [ 24  20  33   0   4   2  15 404 148 350]\n [239  16 400  70 126   0 101   0  45   3]\n [  0   3   0   0   0   0   1 841  24 131]\n [ 10   5  71   9  33   0  13  24 758  77]\n [  1   4  25   0   0   1   1  64  24 880]]\nEpoch 4: Accuracy = 0.6193, F1-score = 0.5773\nConfusion Matrix:\n [[777  17  27  81  27   2  40   2  26   1]\n [ 21 913  10  34   5   0  15   1   1   0]\n [ 26   3 341  10 531   1  68   1  18   1]\n [106  36  17 740  48   0  32   0  21   0]\n [ 12  11 142  81 665   0  63   0  26   0]\n [ 24  15  20   0   3  81  10 381 157 309]\n [251  13 180  70 332   1 104   0  47   2]\n [  0   2   0   0   0   4   0 856  23 115]\n [ 14   2  40   8  29   3  10  22 840  32]\n [  1   1  18   0   0   5   0  70  29 876]]\nEpoch 5: Accuracy = 0.6559, F1-score = 0.6375\nConfusion Matrix:\n [[783  18  30  77  12   2  51   2  24   1]\n [ 14 925   7  29   4   1  19   1   0   0]\n [ 26   3 443  12 417   1  81   1  15   1]\n [ 89  44  28 768  16   0  43   0  12   0]\n [  8  15 241  80 549   1  87   0  19   0]\n [ 14   4  11   0   0 374   4 278  93 222]\n [256  12 257  68 224   4 138   0  39   2]\n [  0   1   0   0   0  23   0 864  10 102]\n [ 16   2  37   7  23  14  15  21 847  18]\n [  1   0  20   0   0  22   0  75  14 868]]\nEpoch 6: Accuracy = 0.6855, F1-score = 0.6721\nConfusion Matrix:\n [[768  19  29  85  13   5  55   2  23   1]\n [ 13 928  10  29   4   1  14   1   0   0]\n [ 21   3 447  12 427   4  70   0  15   1]\n [ 74  38  22 790  19   1  46   0  10   0]\n [  6  13 201  74 600   5  82   0  19   0]\n [  8   2  11   0   0 615   2 164  59 139]\n [244   8 250  67 250   5 137   0  36   3]\n [  0   1   0   0   0  65   0 833   5  96]\n [ 18   1  31   6  18  19  11  13 870  13]\n [  1   0  19   0   0  36   0  65  12 867]]\nEpoch 7: Accuracy = 0.6967, F1-score = 0.6840\nConfusion Matrix:\n [[782  16  24  73  16   5  61   1  21   1]\n [ 11 932  10  28   4   1  14   0   0   0]\n [ 23   2 408   9 467   3  76   0  11   1]\n [ 85  32   6 782  39   1  45   0  10   0]\n [  7  13 131  63 675   5  93   0  13   0]\n [  7   2   9   0   0 659   1 146  45 131]\n [256   8 192  56 303   6 146   0  30   3]\n [  0   1   0   0   0  58   0 840   2  99]\n [ 17   1  27   6  20  25  14  12 869   9]\n [  1   0  18   0   0  30   1  69   7 874]]\nEpoch 8: Accuracy = 0.7073, F1-score = 0.6974\nConfusion Matrix:\n [[775  13  18  77  14   5  73   1  24   0]\n [  8 930  10  28   3   2  19   0   0   0]\n [ 19   2 415   9 445   2  96   0  11   1]\n [ 69  28   4 803  36   1  49   0  10   0]\n [  5  11 132  63 664   4 107   0  14   0]\n [  7   1   7   0   0 697   1 123  39 125]\n [237   7 162  63 302   7 187   0  32   3]\n [  0   1   0   0   0  57   0 838   2 102]\n [ 18   0  22   7  15  20  16  10 883   9]\n [  1   0  18   0   0  33   0  62   5 881]]\nEpoch 9: Accuracy = 0.7144, F1-score = 0.7049\nConfusion Matrix:\n [[783  13  18  73  14   5  71   1  22   0]\n [  8 933  10  26   3   2  18   0   0   0]\n [ 22   2 406  10 445   4 101   0   9   1]\n [ 71  29   4 806  32   1  47   0  10   0]\n [  4  13 120  61 674   4 110   0  14   0]\n [  7   1   6   0   0 732   1 110  37 106]\n [240   8 145  56 309   8 201   0  31   2]\n [  0   0   0   0   0  63   0 837   2  98]\n [ 17   0  21   7  15  23  16   7 889   5]\n [  1   0  17   0   0  33   0  63   3 883]]\nTotal Training Time: 87.27 seconds\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}