{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":283795,"sourceType":"datasetVersion","datasetId":118250}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport time\nfrom sklearn.preprocessing import OneHotEncoder\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T23:50:59.162237Z","iopub.execute_input":"2025-03-05T23:50:59.162625Z","iopub.status.idle":"2025-03-05T23:51:14.340226Z","shell.execute_reply.started":"2025-03-05T23:50:59.162596Z","shell.execute_reply":"2025-03-05T23:51:14.339186Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class SingleLayerNN:\n    def __init__(self, input_size, output_size, learning_rate=0.01):\n        self.weights = np.random.randn(input_size, output_size) * 0.01  # Small random values\n        self.bias = np.zeros((1, output_size))\n        self.learning_rate = learning_rate\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)  # Derivative of sigmoid\n\n    def forward(self, X):\n        self.input = X\n        self.z = np.dot(X, self.weights) + self.bias  # Linear transformation\n        self.output = self.sigmoid(self.z)  # Apply activation\n        return self.output\n\n    def backward(self, y_true):\n        error = self.output - y_true  # Error in prediction\n        d_output = error * self.sigmoid_derivative(self.output)  # Delta (Gradient)\n        \n        d_weights = np.dot(self.input.T, d_output)  # Weight gradient\n        d_bias = np.sum(d_output, axis=0, keepdims=True)  # Bias gradient\n\n        # Update weights and bias using gradient descent\n        self.weights -= self.learning_rate * d_weights\n        self.bias -= self.learning_rate * d_bias\n\n        loss = np.mean(error**2)  # Mean Squared Error\n        return loss\n\n    def train(self, X, y, epochs=100, X_test=None, y_test=None):\n        start_time = time.time()\n        \n        for epoch in range(epochs):\n            output = self.forward(X)\n            loss = self.backward(y)\n            \n            if epoch % 10 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n        \n        end_time = time.time()\n        print(f\"Total Training Time: {end_time - start_time:.2f} seconds\")\n        \n        if X_test is not None and y_test is not None:\n            self.evaluate(X_test, y_test)\n\n    def predict(self, X):\n        return (self.forward(X) > 0.5).astype(int)  # Convert probabilities to class labels\n    \n    def evaluate(self, X_test, y_test):\n        y_pred = self.predict(X_test)\n        \n        accuracy = accuracy_score(y_test, np.argmax(y_pred, axis=1))\n        f1 = f1_score(y_test, np.argmax(y_pred, axis=1), average='macro')\n        conf_matrix = confusion_matrix(y_test, np.argmax(y_pred, axis=1))\n        \n        print(f\"Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}\")\n        print(\"Confusion Matrix:\\n\", conf_matrix)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:11:29.133313Z","iopub.execute_input":"2025-03-06T00:11:29.133719Z","iopub.status.idle":"2025-03-06T00:11:29.144581Z","shell.execute_reply.started":"2025-03-06T00:11:29.133688Z","shell.execute_reply":"2025-03-06T00:11:29.143514Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\n# Paths to dataset\ntrain_path = \"/kaggle/input/cifar10-pngs-in-folders/cifar10/train\"\ntest_path = \"/kaggle/input/cifar10-pngs-in-folders/cifar10/test\"\n\n# Function to load CIFAR-10 images and labels\ndef load_cifar10(dataset_path):\n    categories = sorted(os.listdir(dataset_path))  # Sort categories for consistency\n    label_map = {category: idx for idx, category in enumerate(categories)}\n\n    images, labels = [], []\n    for category in categories:\n        category_path = os.path.join(dataset_path, category)\n        for img_file in os.listdir(category_path):\n            img_path = os.path.join(category_path, img_file)\n            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n            img = cv2.resize(img, (32, 32))  # Ensure 32x32 size\n            images.append(img.flatten() / 255.0)  # Normalize and flatten\n            labels.append(label_map[category])\n\n    return np.array(images), np.array(labels)\n\n# Load dataset\nX_train, y_train = load_cifar10(train_path)\nX_test, y_test = load_cifar10(test_path)\n\n# One-hot encode labels\none_hot_encoder = OneHotEncoder(sparse=False)\ny_train_onehot = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\ny_test_onehot = one_hot_encoder.transform(y_test.reshape(-1, 1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:05:55.850568Z","iopub.execute_input":"2025-03-06T00:05:55.850924Z","iopub.status.idle":"2025-03-06T00:07:07.417314Z","shell.execute_reply.started":"2025-03-06T00:05:55.850897Z","shell.execute_reply":"2025-03-06T00:07:07.416245Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\n# Initialize and train model\ninput_size = 32 * 32  # Image size after flattening\noutput_size = 10  # Number of classes\n\nnn = SingleLayerNN(input_size, output_size, learning_rate=0.1)\nnn.train(X_train, y_train_onehot, epochs=100)\n\n# Evaluate model\nnn.evaluate(X_test, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:11:41.070342Z","iopub.execute_input":"2025-03-06T00:11:41.070717Z","iopub.status.idle":"2025-03-06T00:12:00.823824Z","shell.execute_reply.started":"2025-03-06T00:11:41.070690Z","shell.execute_reply":"2025-03-06T00:12:00.822548Z"}},"outputs":[{"name":"stdout","text":"Epoch 0, Loss: 0.2586\nEpoch 10, Loss: 0.1000\nEpoch 20, Loss: 0.1000\nEpoch 30, Loss: 0.1000\nEpoch 40, Loss: 0.1000\nEpoch 50, Loss: 0.1000\nEpoch 60, Loss: 0.1000\nEpoch 70, Loss: 0.1000\nEpoch 80, Loss: 0.1000\nEpoch 90, Loss: 0.1000\nTotal Training Time: 19.72 seconds\nAccuracy: 0.1000, F1-score: 0.0182\nConfusion Matrix:\n [[1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]\n [1000    0    0    0    0    0    0    0    0    0]]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"class Multilayer_Perceptron(object):\n    def __init__(self, sizes):\n        self.layers = len(sizes)\n        self.sizes = sizes\n        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n        self.weights = [np.random.randn(y,x) for x, y in zip(sizes[:-1], sizes[1:])]\n\n\n    def feedforward(self, a):\n        for b, w in zip(self.biases, self.weights):\n            a = self.sigmoid(np.dot(w,a)+b)\n        return a\n\n    def backpropagation(self, x, y):\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        activation = x\n        activation_list = [x]\n        z_list = []\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation)+b\n            z_list.append(z)\n            activation = self.sigmoid(z)\n            activation_list.append(activation)\n\n        delta = self.cost_derivative(activation_list[-1], y) * self.sigmoid_prime(z_list[-1]) \n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activation_list[-2].transpose())\n\n        for layer in range(2, self.layers):\n            z = z_list[-layer]\n            sp = self.sigmoid_prime(z)\n            delta = np.dot(self.weights[-layer+1].transpose(), delta)*sp\n            nabla_b[-layer] = delta\n            nabla_w[-layer] = np.dot(delta, activation_list[-layer-1].transpose())\n\n        return (nabla_b, nabla_w)\n\n\n    def update_mini_batch(self, mini_batch, eta):\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w = self.backpropagation(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n\n        self.weights = [w-(eta/len(mini_batch))*nw for w,nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n\n\n    def Stochastic_Gradient_Descent(self, epochs, training_data, mini_batch_size, eta, test_data=None):\n        n= len(training_data)\n        \n        if test_data:\n            test_n = len(test_data)\n            #n = len(training_data)\n        start_time = time.time()\n        for j in range(epochs):\n            random.shuffle(training_data)\n            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(mini_batch, eta)\n\n            if test_data:\n                #print('here')\n                #print(\"Epoch{0}: {1}/{2}\".format(j, self.evaluate(test_data), test_n))\n                accuracy, f1, conf_matrix = self.evaluate(test_data)\n                print(f\"Epoch {j}: Accuracy = {accuracy:.4f}, F1-score = {f1:.4f}\")\n                print(\"Confusion Matrix:\\n\", conf_matrix)\n       \n            else:\n                #print('code is here')\n                print(\"Epoch{0} complete\".format(j))\n        end_time = time.time()\n        total_time = end_time-start_time\n        print(f\"Total Training Time: {end_time - start_time:.2f} seconds\")\n\n    def cost_derivative(self, output_activations, y):\n        return (output_activations-y)\n    \n    def sigmoid(self, z):\n        return 1/(1+np.exp(-z))\n\n    def sigmoid_prime(self, z):\n        result = self.sigmoid(z)*(1-self.sigmoid(z))\n        return result\n\n\n    def evaluate(self, test_data):\n        y_pred = []\n        y_true = []\n        for x, y in test_data:\n            predicted = np.argmax(self.feedforward(x))\n            actual = np.argmax(y)\n            y_pred.append(predicted)\n            y_true.append(actual)\n        #test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n        #return sum(int(x == y) for (x, y) in test_results)\n        accuracy = accuracy_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred, average=\"macro\")\n        conf_matrix = confusion_matrix(y_true, y_pred)\n\n        return accuracy, f1, conf_matrix\n\n\n        \n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:12:16.981724Z","iopub.execute_input":"2025-03-06T00:12:16.982099Z","iopub.status.idle":"2025-03-06T00:12:16.999152Z","shell.execute_reply.started":"2025-03-06T00:12:16.982068Z","shell.execute_reply":"2025-03-06T00:12:16.997985Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Normalize pixel values to [0,1]\nx_train = x_train.astype(np.float32) / 255.0\nx_test = x_test.astype(np.float32) / 255.0\n\n# Flatten images into vectors\nx_train = x_train.reshape(x_train.shape[0], -1, 1)  # (50000, 3072, 1)\nx_test = x_test.reshape(x_test.shape[0], -1, 1)    # (10000, 3072, 1)\n\n# One-hot encode labels\ndef one_hot_encode(y, num_classes=10):\n    return np.eye(num_classes)[y].reshape(-1, num_classes, 1)\n\ny_train = one_hot_encode(y_train)\ny_test = one_hot_encode(y_test)\n\n# Prepare training and test data\ntd = list(zip(x_train, y_train))\ntest_data = list(zip(x_test, y_test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:12:29.298914Z","iopub.execute_input":"2025-03-06T00:12:29.299263Z","iopub.status.idle":"2025-03-06T00:12:45.440482Z","shell.execute_reply.started":"2025-03-06T00:12:29.299234Z","shell.execute_reply":"2025-03-06T00:12:45.439301Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"mlp = Multilayer_Perceptron([3072, 128, 64, 10])\nmlp.Stochastic_Gradient_Descent(epochs=10, training_data=td, mini_batch_size=32, eta=0.01, test_data=test_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:12:45.441900Z","iopub.execute_input":"2025-03-06T00:12:45.442270Z","iopub.status.idle":"2025-03-06T00:25:42.088363Z","shell.execute_reply.started":"2025-03-06T00:12:45.442235Z","shell.execute_reply":"2025-03-06T00:25:42.087267Z"}},"outputs":[{"name":"stdout","text":"Epoch 0: Accuracy = 0.1000, F1-score = 0.0182\nConfusion Matrix:\n [[   0    0    0 1000    0    0    0    0    0    0]\n [   0    0    0 1000    0    0    0    0    0    0]\n [   0    0    0 1000    0    0    0    0    0    0]\n [   0    0    0 1000    0    0    0    0    0    0]\n [   0    0    0  998    0    1    0    0    0    1]\n [   0    0    0 1000    0    0    0    0    0    0]\n [   0    0    0  999    0    0    0    1    0    0]\n [   0    0    0 1000    0    0    0    0    0    0]\n [   0    0    0 1000    0    0    0    0    0    0]\n [   0    0    0  999    0    0    0    1    0    0]]\nEpoch 1: Accuracy = 0.0999, F1-score = 0.0184\nConfusion Matrix:\n [[   0    0    0 1000    0    0    0    0    0    0]\n [   0    0    0  998    0    0    0    2    0    0]\n [   0    1    0  999    0    0    0    0    0    0]\n [   0    0    0  998    0    0    0    1    0    1]\n [   0    0    0  998    0    1    0    0    0    1]\n [   0    0    0  998    0    0    0    2    0    0]\n [   0    0    0  998    0    1    0    1    0    0]\n [   0    0    0  999    0    0    0    1    0    0]\n [   0    0    0  999    0    0    0    1    0    0]\n [   0    0    0  999    0    0    0    1    0    0]]\nEpoch 2: Accuracy = 0.1040, F1-score = 0.0892\nConfusion Matrix:\n [[ 80 212  15 278  19  65 210  96   7  18]\n [ 79 184  30 180  32 149 135 134  40  37]\n [121 102  17 306  32  56 181 134  19  32]\n [ 65 131  47 270  40  91 137 164  23  32]\n [ 83 109  23 288  52  69 156 162  15  43]\n [ 90 141  38 277  37  87 114 158  19  39]\n [ 87  84  34 239  39  65 149 226  26  51]\n [ 89 121  28 308  34  64 134 154  18  50]\n [ 92 287  11 210  21  75 150 126   8  20]\n [ 95 238  16 198  29  71 138 160  16  39]]\nEpoch 3: Accuracy = 0.1051, F1-score = 0.0919\nConfusion Matrix:\n [[121 234  13 214  21  72 238  57  11  19]\n [100 200  24 149  34 158 154  95  43  43]\n [162 119  15 247  35  64 205  96  22  35]\n [108 141  46 230  40  93 153 126  26  37]\n [128 123  16 242  51  78 167 122  23  50]\n [123 165  40 232  36  94 130 116  21  43]\n [141  98  29 194  41  77 162 167  32  59]\n [132 136  26 251  34  73 150 122  17  59]\n [133 305   9 166  22  86 158  88  10  23]\n [123 264  15 178  26  76 140 112  20  46]]\nEpoch 4: Accuracy = 0.1084, F1-score = 0.0943\nConfusion Matrix:\n [[182 217  12 194  18  72 228  41  13  23]\n [114 207  17 135  33 160 157  87  45  45]\n [210 116  11 216  32  70 204  80  20  41]\n [146 147  42 210  37  92 151 107  28  40]\n [187 119  13 219  49  71 161 101  24  56]\n [166 167  32 204  33  99 137  87  27  48]\n [182 108  25 177  37  79 167 137  33  55]\n [181 134  20 230  30  68 149 100  20  68]\n [168 300   8 140  22  86 163  77  13  23]\n [150 286  13 160  23  79 137  88  18  46]]\nEpoch 5: Accuracy = 0.1134, F1-score = 0.0975\nConfusion Matrix:\n [[250 208  12 161  19  69 209  32  13  27]\n [138 213  16 125  30 162 151  73  41  51]\n [276 119  10 183  27  73 195  57  18  42]\n [173 141  32 193  36  96 154  94  31  50]\n [232 123  11 201  47  65 158  81  27  55]\n [198 166  30 187  30 102 127  74  29  57]\n [233 110  21 154  37  71 165 118  31  60]\n [223 127  18 210  28  74 137  90  21  72]\n [212 286   7 128  17  89 159  64  13  25]\n [178 289  11 153  20  80 128  74  16  51]]\nEpoch 6: Accuracy = 0.1170, F1-score = 0.0990\nConfusion Matrix:\n [[296 193   5 143  19  66 200  29  16  33]\n [168 213  12 113  30 155 145  69  41  54]\n [328 106   7 164  24  72 191  46  20  42]\n [209 141  30 179  31  96 146  83  31  54]\n [279 111   9 185  44  71 153  65  27  56]\n [217 162  27 176  33 106 122  69  28  60]\n [262 105  14 136  34  71 175 100  32  71]\n [246 127  16 195  28  74 137  84  18  75]\n [242 270   4 117  18  96 155  57  11  30]\n [213 276   9 143  16  77 127  66  18  55]]\nEpoch 7: Accuracy = 0.1189, F1-score = 0.1003\nConfusion Matrix:\n [[330 184   3 123  17  64 198  27  19  35]\n [183 211   9 102  28 155 142  67  42  61]\n [353  98   8 145  24  68 190  42  24  48]\n [236 137  27 163  34  95 148  67  35  58]\n [296 116   8 172  40  64 150  59  32  63]\n [231 157  25 162  29 110 123  64  30  69]\n [276 107  13 121  35  74 172  93  35  74]\n [259 124  15 184  28  78 128  78  20  86]\n [260 277   5 105  14  91 154  47  15  32]\n [228 279   8 125  14  73 125  62  24  62]]\nEpoch 8: Accuracy = 0.1222, F1-score = 0.1033\nConfusion Matrix:\n [[357 178   3  99  18  66 196  26  20  37]\n [182 214   8  96  26 144 140  67  55  68]\n [363  98   7 128  22  69 187  47  29  50]\n [240 136  28 145  35  90 148  65  44  69]\n [306 109   8 154  42  60 156  58  37  70]\n [240 153  22 148  31 114 122  65  30  75]\n [285 107  14 101  34  75 173  94  37  80]\n [273 124  11 167  28  74 127  77  25  94]\n [282 264   3  91  16  90 153  43  19  39]\n [231 268   9 110  13  77 128  60  30  74]]\nEpoch 9: Accuracy = 0.1247, F1-score = 0.1058\nConfusion Matrix:\n [[362 167   3  98  17  60 198  26  24  45]\n [175 213   7  98  26 130 140  62  66  83]\n [365 100   7 122  24  60 189  44  34  55]\n [238 128  26 155  35  90 149  59  49  71]\n [293 108   8 150  41  60 167  59  41  73]\n [243 143  20 152  28 108 123  62  37  84]\n [279 106  16 103  33  70 181  93  40  79]\n [272 116  11 167  30  73 130  74  29  98]\n [308 235   3  94  15  87 149  41  23  45]\n [232 250   9 110  13  74 133  60  36  83]]\nTotal Training Time: 776.62 seconds\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}