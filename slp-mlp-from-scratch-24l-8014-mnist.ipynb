{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34877,"sourceType":"datasetVersion","datasetId":27352}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:45:09.630333Z","iopub.execute_input":"2025-03-06T00:45:09.630676Z","iopub.status.idle":"2025-03-06T00:45:10.047535Z","shell.execute_reply.started":"2025-03-06T00:45:09.630648Z","shell.execute_reply":"2025-03-06T00:45:10.046300Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mnist-in-csv/mnist_test.csv\n/kaggle/input/mnist-in-csv/mnist_train.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport random\nimport time\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:45:10.049007Z","iopub.execute_input":"2025-03-06T00:45:10.049904Z","iopub.status.idle":"2025-03-06T00:45:10.768590Z","shell.execute_reply.started":"2025-03-06T00:45:10.049835Z","shell.execute_reply":"2025-03-06T00:45:10.767430Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class SingleLayerNN:\n    def __init__(self, input_size, output_size, learning_rate=0.01):\n        self.weights = np.random.randn(input_size, output_size) * 0.01  # Small random values\n        self.bias = np.zeros((1, output_size))\n        self.learning_rate = learning_rate\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)  # Derivative of sigmoid\n\n    def forward(self, X):\n        self.input = X\n        self.z = np.dot(X, self.weights) + self.bias  # Linear transformation\n        self.output = self.sigmoid(self.z)  # Apply activation\n        return self.output\n\n    def backward(self, y_true):\n        error = self.output - y_true  # Error in prediction\n        d_output = error * self.sigmoid_derivative(self.output)  # Delta (Gradient)\n        \n        d_weights = np.dot(self.input.T, d_output)  # Weight gradient\n        d_bias = np.sum(d_output, axis=0, keepdims=True)  # Bias gradient\n\n        # Update weights and bias using gradient descent\n        self.weights -= self.learning_rate * d_weights\n        self.bias -= self.learning_rate * d_bias\n\n        loss = np.mean(error**2)  # Mean Squared Error\n        return loss\n\n    def train(self, X, y, epochs=100, X_test=None, y_test=None):\n        start_time = time.time()\n        \n        for epoch in range(epochs):\n            output = self.forward(X)\n            loss = self.backward(y)\n            \n            if epoch % 10 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n        \n        end_time = time.time()\n        print(f\"Total Training Time: {end_time - start_time:.2f} seconds\")\n        \n        if X_test is not None and y_test is not None:\n            self.evaluate(X_test, y_test)\n\n    def predict(self, X):\n        return (self.forward(X) > 0.5).astype(int)  # Convert probabilities to class labels\n    \n    def evaluate(self, X_test, y_test):\n        y_pred = self.predict(X_test)\n        \n        accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n        f1 = f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='macro')\n        conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n        \n        print(f\"Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}\")\n        print(\"Confusion Matrix:\\n\", conf_matrix)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:45:10.770243Z","iopub.execute_input":"2025-03-06T00:45:10.770779Z","iopub.status.idle":"2025-03-06T00:45:10.783672Z","shell.execute_reply.started":"2025-03-06T00:45:10.770723Z","shell.execute_reply":"2025-03-06T00:45:10.782353Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/mnist-in-csv/mnist_train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/mnist-in-csv/mnist_test.csv\")\n\n# **2. Preprocess Data**\nX_train = train_data.iloc[:, 1:].values / 255.0  \ny_train = train_data.iloc[:, 0].values.reshape(-1, 1)  \n\nX_test = test_data.iloc[:, 1:].values / 255.0  \ny_test = test_data.iloc[:, 0].values.reshape(-1, 1)  \n\n# **3. One-Hot Encoding for Labels**\nencoder = OneHotEncoder(sparse=False)\ny_train_onehot = encoder.fit_transform(y_train)\ny_test_onehot = encoder.transform(y_test)\n\n# **4. Initialize Model**\ninput_size = X_train.shape[1]  \noutput_size = y_train_onehot.shape[1]  \nmodel = SingleLayerNN(input_size, output_size, learning_rate=0.1)\n\n# **5. Train Model**\nmodel.train(X_train, y_train_onehot, epochs=100, X_test=X_test, y_test=y_test)\n\n# **6. Evaluate Model**\n# y_pred = model.predict(X_test)\n# y_pred = np.argmax(model.forward(X_test), axis=1)  # Convert one-hot predictions to class labels\n# y_test = y_test.ravel()  # Ensure y_test is a 1D array\n\n# accuracy = accuracy_score(y_test, y_pred)\n# f1 = f1_score(y_test, y_pred, average='macro')\n# #conf_matrix = confusion_matrix(y_test, y_pred)\n\n# print(f\"Accuracy: {accuracy:.4f}\")\n# print(f\"F1 Score: {f1:.4f}\")\n# #print(\"Confusion Matrix:\")\n# #print(conf_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:45:10.785550Z","iopub.execute_input":"2025-03-06T00:45:10.785949Z","iopub.status.idle":"2025-03-06T00:45:35.981951Z","shell.execute_reply.started":"2025-03-06T00:45:10.785912Z","shell.execute_reply":"2025-03-06T00:45:35.980989Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Loss: 0.2493\nEpoch 10, Loss: 0.1000\nEpoch 20, Loss: 0.1000\nEpoch 30, Loss: 0.1000\nEpoch 40, Loss: 0.1000\nEpoch 50, Loss: 0.1000\nEpoch 60, Loss: 0.1000\nEpoch 70, Loss: 0.1000\nEpoch 80, Loss: 0.1000\nEpoch 90, Loss: 0.1000\nTotal Training Time: 19.20 seconds\nAccuracy: 1.0000, F1-score: 1.0000\nConfusion Matrix:\n [[10000]]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# model = SingleLayerNN(input_size=784, output_size=1, learning_rate=0.1)\n# model.train(X_train, y_train, epochs=100, X_test=X_test, y_test=y_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:45:35.982864Z","iopub.execute_input":"2025-03-06T00:45:35.983155Z","iopub.status.idle":"2025-03-06T00:45:35.986805Z","shell.execute_reply.started":"2025-03-06T00:45:35.983132Z","shell.execute_reply":"2025-03-06T00:45:35.985980Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class Multilayer_Perceptron(object):\n    def __init__(self, sizes):\n        self.layers = len(sizes)\n        self.sizes = sizes\n        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n        self.weights = [np.random.randn(y,x) for x, y in zip(sizes[:-1], sizes[1:])]\n\n\n    def feedforward(self, a):\n        for b, w in zip(self.biases, self.weights):\n            a = self.sigmoid(np.dot(w,a)+b)\n        return a\n\n    def backpropagation(self, x, y):\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        activation = x\n        activation_list = [x]\n        z_list = []\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation)+b\n            z_list.append(z)\n            activation = self.sigmoid(z)\n            activation_list.append(activation)\n\n        delta = self.cost_derivative(activation_list[-1], y) * self.sigmoid_prime(z_list[-1]) \n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activation_list[-2].transpose())\n\n        for layer in range(2, self.layers):\n            z = z_list[-layer]\n            sp = self.sigmoid_prime(z)\n            delta = np.dot(self.weights[-layer+1].transpose(), delta)*sp\n            nabla_b[-layer] = delta\n            nabla_w[-layer] = np.dot(delta, activation_list[-layer-1].transpose())\n\n        return (nabla_b, nabla_w)\n\n\n    def update_mini_batch(self, mini_batch, eta):\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w = self.backpropagation(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n\n        self.weights = [w-(eta/len(mini_batch))*nw for w,nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n\n\n    def Stochastic_Gradient_Descent(self, epochs, training_data, mini_batch_size, eta, test_data=None):\n        n= len(training_data)\n        \n        if test_data:\n            test_n = len(test_data)\n            #n = len(training_data)\n        start_time = time.time()\n        for j in range(epochs):\n            random.shuffle(training_data)\n            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(mini_batch, eta)\n\n            if test_data:\n                #print('here')\n                #print(\"Epoch{0}: {1}/{2}\".format(j, self.evaluate(test_data), test_n))\n                accuracy, f1, conf_matrix = self.evaluate(test_data)\n                print(f\"Epoch {j}: Accuracy = {accuracy:.4f}, F1-score = {f1:.4f}\")\n                print(\"Confusion Matrix:\\n\", conf_matrix)\n       \n            else:\n                #print('code is here')\n                print(\"Epoch{0} complete\".format(j))\n        end_time = time.time()\n        total_time = end_time-start_time\n        print(f\"Total Training Time: {end_time - start_time:.2f} seconds\")\n\n    def cost_derivative(self, output_activations, y):\n        return (output_activations-y)\n    \n    def sigmoid(self, z):\n        return 1/(1+np.exp(-z))\n\n    def sigmoid_prime(self, z):\n        result = self.sigmoid(z)*(1-self.sigmoid(z))\n        return result\n\n\n    def evaluate(self, test_data):\n        y_pred = []\n        y_true = []\n        for x, y in test_data:\n            predicted = np.argmax(self.feedforward(x))\n            actual = np.argmax(y)\n            y_pred.append(predicted)\n            y_true.append(actual)\n        #test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n        #return sum(int(x == y) for (x, y) in test_results)\n        accuracy = accuracy_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred, average=\"macro\")\n        conf_matrix = confusion_matrix(y_true, y_pred)\n\n        return accuracy, f1, conf_matrix\n\n\n        \n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:45:35.987979Z","iopub.execute_input":"2025-03-06T00:45:35.988272Z","iopub.status.idle":"2025-03-06T00:45:36.011680Z","shell.execute_reply.started":"2025-03-06T00:45:35.988240Z","shell.execute_reply":"2025-03-06T00:45:36.010271Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n# Load dataset\ntrain_data = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\ntest_data = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n\n# Split into features (X) and labels (y)\nX_train = train_data.iloc[:, 1:].values / 255.0  # Normalize pixel values\ny_train = train_data.iloc[:, 0].values  # Labels (0-9)\n\nX_test = test_data.iloc[:, 1:].values / 255.0\ny_test = test_data.iloc[:, 0].values\n\n# Convert labels to one-hot encoding\ndef one_hot_encode(y, num_classes=10):\n    one_hot = np.zeros((len(y), num_classes))\n    one_hot[np.arange(len(y)), y] = 1\n    return one_hot\n\ny_train_oh = one_hot_encode(y_train)\ny_test_oh = one_hot_encode(y_test)\n\n# Reshape inputs to column vectors\nX_train = [x.reshape(-1, 1) for x in X_train]\ny_train_oh = [y.reshape(-1, 1) for y in y_train_oh]\nX_test = [x.reshape(-1, 1) for x in X_test]\ny_test_oh = [y.reshape(-1, 1) for y in y_test_oh]\n\n# Combine into tuples\ntraining_data = list(zip(X_train, y_train_oh))\ntest_data = list(zip(X_test, y_test_oh))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:45:36.013457Z","iopub.execute_input":"2025-03-06T00:45:36.013898Z","iopub.status.idle":"2025-03-06T00:45:40.381198Z","shell.execute_reply.started":"2025-03-06T00:45:36.013870Z","shell.execute_reply":"2025-03-06T00:45:40.379878Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define model architecture: [784 input nodes, 128 hidden nodes, 10 output nodes]\nmlp = Multilayer_Perceptron([784, 64, 16, 10])\n\n# Train the model\nmlp.Stochastic_Gradient_Descent(\n    epochs=10, \n    training_data=training_data, \n    mini_batch_size=32, \n    eta=0.1, \n    test_data=test_data\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T00:45:40.383280Z","iopub.execute_input":"2025-03-06T00:45:40.383566Z","iopub.status.idle":"2025-03-06T00:48:18.519636Z","shell.execute_reply.started":"2025-03-06T00:45:40.383544Z","shell.execute_reply":"2025-03-06T00:48:18.518419Z"}},"outputs":[{"name":"stdout","text":"Epoch 0: Accuracy = 0.3574, F1-score = 0.3007\nConfusion Matrix:\n [[242  35  77  87 117 144   2 217   9  50]\n [ 29 896   7 108  12   6   2   2  37  36]\n [ 47 205 481 113  45  25   7  93  10   6]\n [ 49  40  46 675  61  38   1  45  50   5]\n [145  55  15  49 245 109   2 208  95  59]\n [181  44  20 248  54 155   1  69  93  27]\n [266 115  61  23 213  82   0 100  68  30]\n [ 35  52  78  29  40  25   1 744   9  15]\n [166 220  30 192 123  39   6  49 117  32]\n [182  51  19  38 279  25   0 292 104  19]]\nEpoch 1: Accuracy = 0.4733, F1-score = 0.4113\nConfusion Matrix:\n [[ 600   14   71   54   67   53    0   48    9   64]\n [   2 1063    5   29    5    1    0    1   11   18]\n [  50  106  669   81   32   13    3   57    9   12]\n [  38   24   57  744   40   32    0   30   32   13]\n [ 118   35   16   21  452   40    0  101   49  150]\n [ 216   39   19  245   58  161    1   44   66   43]\n [ 238   97  104   20  209   29    0   35   73  153]\n [  26   37   74   17   47   11    0  780    5   31]\n [ 129  207   46  182  126   34    0   28  136   86]\n [ 164   27   13   29  368   10    0  201   69  128]]\nEpoch 2: Accuracy = 0.5579, F1-score = 0.4998\nConfusion Matrix:\n [[ 758    7   56   31   19   30    0   31   13   35]\n [   1 1093    8    4    4    1    1    1   11   11]\n [  46   70  742   66   24   11    0   43   12   18]\n [  31   14   56  771   16   34    0   33   33   22]\n [  49   22   16   16  532   13    0   68   19  247]\n [ 202   27   25  210   46  209    0   45   70   58]\n [ 208   67  126   16  183   23    1   29   77  228]\n [  11   33   47   14   36   10    0  812    5   60]\n [  92  169   46  154  117   63    0   24  173  136]\n [  63   12   11   20  260   10    0  113   32  488]]\nEpoch 3: Accuracy = 0.6131, F1-score = 0.5579\nConfusion Matrix:\n [[ 850    1   38   15    7   19    0   20   14   16]\n [   0 1100   10    2    6    1    0    1    7    8]\n [  51   46  783   55   27   10    0   31   16   13]\n [  31   13   45  792    7   40    0   34   29   19]\n [  25   11   15   13  583   13    1   61   11  249]\n [ 165   19   25  173   44  298    1   42   76   49]\n [ 215   59  141   15  189   25    0   28   76  210]\n [   9   24   46   14   25    5    0  838    8   59]\n [  76  129   45  145  107   80    0   25  242  125]\n [  36    9    8   18  183   17    0   78   15  645]]\nEpoch 4: Accuracy = 0.6481, F1-score = 0.5995\nConfusion Matrix:\n [[ 885    1   24   11    4   18    1   13   17    6]\n [   0 1103    9    3    5    0    1    1   10    3]\n [  41   41  798   52   26    8    5   29   22   10]\n [  27   13   38  803    4   34    0   32   42   17]\n [  21    9   11   10  602   13    2   45   17  252]\n [  99   14   25  147   40  364    3   38  124   38]\n [ 200   54  148   12  199   30    4   25   97  189]\n [   9   21   43   14   26    3    0  844   11   57]\n [  53  103   35  121   83   91    0   23  373   92]\n [  29    6    6   13  148   20    0   58   24  705]]\nEpoch 5: Accuracy = 0.6907, F1-score = 0.6650\nConfusion Matrix:\n [[ 897    0   20    9    2   17    5   10   18    2]\n [   0 1102    8    4    3    0    1    1   12    4]\n [  34   32  812   51   20    9   18   25   25    6]\n [  24    9   40  814    1   38    2   26   43   13]\n [  16    9   13    9  620   13   16   36   12  238]\n [  80    8   24  137   36  413    8   35  115   36]\n [ 180   44  124   12  142   32  213   19   95   97]\n [   9   15   37   19   22    4    4  847   15   56]\n [  44   75   33  115   65   89    4   23  449   77]\n [  25    4    5   13  113   22   13   50   24  740]]\nEpoch 6: Accuracy = 0.7374, F1-score = 0.7260\nConfusion Matrix:\n [[ 900    0   17    6    1   17   13    8   18    0]\n [   0 1096    8    5    3    0    3    1   16    3]\n [  29   20  817   50   21    9   27   25   31    3]\n [  21    7   39  820    2   42    1   25   42   11]\n [  10    8   11    7  669   10   36   23   11  197]\n [  61    7   21  122   39  456   11   33  113   29]\n [  99   27   72    5   79   27  510   17   71   51]\n [   6   13   31   17   22    4    7  855   19   54]\n [  45   56   28  104   59   69   17   20  520   56]\n [  17    4    5   12  109   18   36   47   30  731]]\nEpoch 7: Accuracy = 0.7641, F1-score = 0.7557\nConfusion Matrix:\n [[ 896    0   17    6    2   14   22    6   17    0]\n [   0 1099    6    5    2    0    4    1   14    4]\n [  28   14  824   43   20    8   40   22   32    1]\n [  21    5   42  819    2   44    3   22   42   10]\n [  10    8   10    7  666   11   55   20   11  184]\n [  54    5   23  113   36  487   20   28  100   26]\n [  65   23   52    2   48   22  659   11   42   34]\n [   6   17   29   16   20    3    9  861   16   51]\n [  45   52   24   86   41   64   23   19  576   44]\n [  16    5    4   11   96   15   36   43   29  754]]\nEpoch 8: Accuracy = 0.7853, F1-score = 0.7786\nConfusion Matrix:\n [[ 907    0   15    5    3   13   16    7   14    0]\n [   0 1089    6    3    3    3    5    1   24    1]\n [  30    8  828   37   23    6   37   25   38    0]\n [  22    5   41  806    2   50    4   24   47    9]\n [  10    7    8    3  733   11   49   19   10  132]\n [  48    2   21   97   38  510   21   28  105   22]\n [  49   18   40    1   34   19  750    8   28   11]\n [   6   12   30   13   21    3    7  878   20   38]\n [  49   39   18   64   40   56   26   22  633   27]\n [  15    4    4   11  126   15   34   51   30  719]]\nEpoch 9: Accuracy = 0.8012, F1-score = 0.7958\nConfusion Matrix:\n [[ 909    0   13    6    1   13   18    7   13    0]\n [   0 1088    6    3    2    4    5    1   24    2]\n [  28    8  825   35   23    6   42   24   40    1]\n [  20    5   37  807    1   55    5   26   44   10]\n [   8    6    7    3  734   12   50   16   10  136]\n [  43    2   17   89   38  552   25   24   81   21]\n [  47   11   35    1   25   19  790    6   17    7]\n [   5   13   28   14   21    3    5  882   18   39]\n [  33   29   16   60   31   55   31   21  672   26]\n [  11    4    4   11  108   15   21   52   30  753]]\nTotal Training Time: 158.13 seconds\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}